{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import pickle\n",
    "import urllib.request\n",
    "from itertools import product\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "\n",
    "raw_data_path='../data/raw_data_all_positions.pkl'\n",
    "parsed_data_path='../data/parsed_data_all_positions.pkl'\n",
    "\n",
    "# PyPI imports\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the data file paths\n",
    "raw_data_path='../data/raw_data_all_positions.pkl'\n",
    "parsed_data_path='../data/parsed_data_all_positions.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def download_url(url: str) -> bytes:\n",
    "    '''Takes string url, downloads URL and returns HTML bytes object'''\n",
    "\n",
    "    headers={\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"httpbin.io\",\n",
    "        \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": '\"Linux\"',\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"cross-site\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Create the request\n",
    "    request_params = urllib.request.Request(\n",
    "        url=url,\n",
    "        headers=headers\n",
    "    )   \n",
    "\n",
    "    # Get the html\n",
    "    with urllib.request.urlopen(request_params) as response:\n",
    "        html=response.read()\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def parse_html_table(html: bytes, year: int, week: int, profile: str) -> pd.DataFrame:\n",
    "    '''Takes a html bytes object from URL, parses data table, adds\n",
    "    year, week, position and scoring profile and returns as pandas dataframe'''\n",
    "\n",
    "    # Extract the table rows\n",
    "    soup=BeautifulSoup(html, 'html.parser')\n",
    "    table=soup.find('table',{'class':'datasmall table'})\n",
    "    table_rows=table.find_all('tr')\n",
    "\n",
    "    # Get the column names from the first row\n",
    "    columns=table_rows[0].find_all('th')\n",
    "    column_names=[column.getText() for column in columns]\n",
    "    column_names.extend(['Year', 'Week', 'Scoring profile'])\n",
    "\n",
    "    # Get the values for each row\n",
    "    data=[]\n",
    "\n",
    "    for row in table_rows[1:]:\n",
    "        columns=row.find_all('td')\n",
    "        values=[column.getText() for column in columns]\n",
    "        values.extend([year, week, profile])\n",
    "        data.append(values)\n",
    "\n",
    "    # Convert to pandas dataframe and return\n",
    "    return pd.DataFrame(columns=column_names, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Main script to download data\n",
    "download_data = False  # Or False, depending on what you want to do\n",
    "\n",
    "if download_data is True:\n",
    "    positions = ['qb', 'rb', 'wr', 'te']\n",
    "    profile = 'p'\n",
    "    years = list(range(2020, 2024))\n",
    "    weeks = list(range(1, 19))\n",
    "\n",
    "    # Empty dict. to store dataframes for each position\n",
    "    position_data={}\n",
    "\n",
    "    # Loop on positions first, and create a separate dataframe for each\n",
    "    for position in positions:\n",
    "\n",
    "        # Empty list to collect data for this position\n",
    "        results = []\n",
    "\n",
    "        for year, week in product(years, weeks):\n",
    "            print(f'Downloading {position.upper()}, {year}, week {week}', end='\\r')\n",
    "            url = f'https://www.footballguys.com/playerhistoricalstats?pos={position}&yr={year}&startwk={week}&stopwk={week}&profile={profile}'\n",
    "            \n",
    "            # Get the HTML\n",
    "            html = download_url(url)\n",
    "            \n",
    "            # Parse the HTML\n",
    "            result = parse_html_table(html, year, week, profile)\n",
    "            \n",
    "            # Collect the result\n",
    "            results.append(result)\n",
    "\n",
    "            # Wait before downloading the next page\n",
    "            time.sleep(randrange(1, 5))\n",
    "\n",
    "        # Combine the week-by-week dataframes\n",
    "        data_df = pd.concat(results)\n",
    "\n",
    "        # Add the dataframe for this position to the collection\n",
    "        position_data[position]=data_df\n",
    "    \n",
    "    # Save the raw data\n",
    "    pickle.dump(position_data, open(raw_data_path, 'wb'))\n",
    "    \n",
    "elif download_data is False:\n",
    "    position_data = pickle.load(open(raw_data_path, 'rb'))\n",
    "    print('Loaded data from file', end='')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the result\n",
    "for position, data_df in position_data.items():\n",
    "    print(f'\\nPosition: {position}\\n')\n",
    "    print(data_df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
